# Positional Encoding in Transformers (PyTorch)

ğŸš€ This project is a minimal implementation of **Positional Encoding** using PyTorch, a key component of Transformer-based models (like BERT, GPT, etc.).

## ğŸ§  What it does
The `PositionalEncoding` module adds information about the **position of tokens** in a sequence using sine and cosine functions of different frequencies â€” allowing the model to understand word order.

## ğŸ“¦ Components
- `PositionalEncoding` class (subclass of `nn.Module`)
- Sinusoidal calculation for even and odd indices
- Shape: `(batch_size, seq_len, d_model)`
- Includes dropout for regularization

## ğŸ”§ Example usage
```python
embedding = nn.Embedding(vocab_size, d_model)
pos_encoder = PositionalEncoding(d_model)

x = embedding(input_tokens)         # shape: (batch_size, seq_len, d_model)
x = pos_encoder(x)                  # adds positional info
```

## ğŸ› ï¸ Requirements
- Python â‰¥ 3.7  
- PyTorch â‰¥ 1.10  
- math

## ğŸ“‚ Structure
```
positional_encoding.ipynb     â† Main implementation notebook
README.md                     â† This file
```

## ğŸ“Œ Inspired by
- "Attention is All You Need" (Vaswani et al., 2017)
- PyTorch official tutorials

---

Feel free to â­ the repo or fork for learning purposes!
